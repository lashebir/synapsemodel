{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # New Approach -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ABRA_v1_2_26 import interpolate_and_smooth, CNN, plot_wave, calculate_and_plot_wave, plot_waves_single_frequency, arfread, get_str, calculate_hearing_threshold, all_thresholds, peak_finding\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_finding_model = CNN()\n",
    "model_loader = torch.load('./models/waveI_cnn_model1.pth')\n",
    "peak_finding_model.load_state_dict(model_loader)\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finding(wave):\n",
    "    # Prepare waveform\n",
    "    waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "    # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "    waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "    # print(waveform_torch)\n",
    "    # Get prediction from model\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "    # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "    # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "    # Find peaks and troughs\n",
    "    n = 18\n",
    "    t = 14\n",
    "    # start_point = prediction - 9 archived ABRA\n",
    "    start_point = prediction - 6 #newer ABRA\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "    sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "    highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "    relevant_troughs = np.array([])\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        c = 0\n",
    "        for t in smoothed_troughs:\n",
    "            if t > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if t < highest_smoothed_peaks[p+1]:\n",
    "                            relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                    break\n",
    "    relevant_troughs = relevant_troughs.astype('i')\n",
    "    return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "def extract_metadata(metadata_lines):\n",
    "    # Dictionary to store extracted metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for line in metadata_lines:\n",
    "        # Extract SW FREQ\n",
    "        freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "        if freq_match:\n",
    "            metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "        # Extract LEVELS\n",
    "        levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "        if levels_match:\n",
    "            # Split levels and convert to list of floats\n",
    "            metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def read_custom_tsv(file_path):\n",
    "    # Read the entire file\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content into metadata and data sections\n",
    "    metadata_lines = []\n",
    "    data_section = None\n",
    "    \n",
    "    # Find the ':DATA' marker\n",
    "    data_start = content.find(':DATA')\n",
    "    \n",
    "    if data_start != -1:\n",
    "        # Extract metadata (lines before ':DATA')\n",
    "        metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "        # Extract data section\n",
    "        data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "    # Extract specific metadata\n",
    "    metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "    # Read the data section directly\n",
    "    try:\n",
    "        # Use StringIO to create a file-like object from the data section\n",
    "        raw_data = pd.read_csv(\n",
    "            io.StringIO(data_section), \n",
    "            sep='\\s+',  # Use whitespace as separator\n",
    "            header=None\n",
    "        )\n",
    "        raw_data = raw_data.T\n",
    "        # Add metadata columns to the DataFrame\n",
    "        if 'SW_FREQ' in metadata:\n",
    "            raw_data['Freq(Hz)'] = metadata['SW_FREQ']\n",
    "            raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "        if 'LEVELS' in metadata:\n",
    "            # Repeat levels to match the number of rows\n",
    "            levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "            raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "        filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "        columns = ['Freq(Hz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "        filtered_data = filtered_data[columns]\n",
    "        return filtered_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def peaks_troughs_amp_final(df, freq, db, time_scale=10):\n",
    "#     khz = df[(df['Freq(Hz)'] == freq) & (df['Level(dB)'] == db)]\n",
    "#     if not khz.empty:\n",
    "#         index = khz.index.values[0]\n",
    "#         final = df.loc[index, '0':].dropna()\n",
    "#         final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "#         target = int(244 * (time_scale / 10))\n",
    "        \n",
    "#         y_values = interpolate_and_smooth(final, target)  # Original y-values for plotting\n",
    "#         sampling_rate = len(y_values) / time_scale\n",
    "\n",
    "#         x_values = np.linspace(0, len(y_values) / sampling_rate, len(y_values))\n",
    "\n",
    "#         y_values = interpolate_and_smooth(final[:244])\n",
    "\n",
    "#         fpf = df[(df['Freq(Hz)'] == freq)].loc[:, '0':]\n",
    "\n",
    "#         # Flatten the data to scale all values across the group\n",
    "#         flattened_data = fpf.values.flatten().reshape(-1, 1)\n",
    "\n",
    "#         # Step 1: Standardize the data\n",
    "#         scaler = StandardScaler()\n",
    "#         standardized_data = scaler.fit_transform(flattened_data)\n",
    "\n",
    "#         # Step 2: Apply min-max scaling\n",
    "#         min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#         scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(fpf.shape)\n",
    "\n",
    "#         # Reshape back to the original shape and update the group\n",
    "#         fpf[fpf.columns] = scaled_data\n",
    "\n",
    "#         finalfpf = fpf.loc[index, '0':].dropna()\n",
    "#         finalfpf = pd.to_numeric(finalfpf, errors='coerce').dropna()\n",
    "\n",
    "#         target = int(244 * (time_scale / 10))\n",
    "        \n",
    "#         y_values_fpf = interpolate_and_smooth(finalfpf, target)  # Original y-values for plotting\n",
    "#         sampling_rate = len(y_values) / time_scale\n",
    "\n",
    "#         y_values_fpf = interpolate_and_smooth(finalfpf[:244])\n",
    "\n",
    "#         highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "\n",
    "#         if highest_peaks.size > 0:  # Check if highest_peaks is not empty\n",
    "#             first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "\n",
    "#         return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "#     return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_troughs_amp_final(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    db_column = 'Level(dB)'\n",
    "    \n",
    "    khz = df[(df['Freq(Hz)'] == freq) & (df[db_column] == db)]\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.values.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "        y_values_fpf = interpolate_and_smooth(scaled_data[:244])\n",
    "        \n",
    "        # Find peaks using the normalized data\n",
    "        highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "        \n",
    "        # Calculate amplitude on the processed but non-normalized data\n",
    "        if highest_peaks.size > 0 and relevant_troughs.size > 0:\n",
    "            # Following the same approach as in the display_metrics_table function\n",
    "            first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "            return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m freq \u001b[38;5;129;01min\u001b[39;00m freqs:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lvl \u001b[38;5;129;01min\u001b[39;00m levels:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# print(\"Frequency=\",freq, \"Level=\", lvl)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         _, _, amp \u001b[38;5;241m=\u001b[39m \u001b[43mpeaks_troughs_amp_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# print(f'Amplitude: {amp}\\n')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         amp_per_freq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(subject)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mpeaks_troughs_amp_final\u001b[0;34m(df, freq, db, time_scale, multiply_y_factor, units)\u001b[0m\n\u001b[1;32m     23\u001b[0m y_values_fpf \u001b[38;5;241m=\u001b[39m interpolate_and_smooth(y_values[:\u001b[38;5;241m244\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Standardize and normalize for peak finding, exactly as in the original\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m flattened_data \u001b[38;5;241m=\u001b[39m \u001b[43my_values_fpf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m     28\u001b[0m standardized_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(flattened_data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "time_scale = 18\n",
    "amp_per_freq = {'Subject': [], 'Freq(Hz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "for subject in os.listdir(start_path):\n",
    "    # print(\"Subject:\",subject)\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        # print(fq)\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            path = os.path.join(start_path,subject,fq)\n",
    "            data_df = read_custom_tsv(path)\n",
    "            freqs = data_df['Freq(Hz)'].unique().tolist()\n",
    "            levels = data_df['Level(dB)'].unique().tolist()\n",
    "            for freq in freqs:\n",
    "                for lvl in levels:\n",
    "                    # print(\"Frequency=\",freq, \"Level=\", lvl)\n",
    "                    _, _, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl, time_scale=time_scale)\n",
    "                    # print(f'Amplitude: {amp}\\n')\n",
    "                    amp_per_freq['Subject'].append(subject)\n",
    "                    amp_per_freq['Freq(Hz) (x1)'].append(freq)\n",
    "                    amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "                    amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna()\n",
    "raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Freq':'Freq(Hz) (x1)'}, inplace=True)\n",
    "raw_synapse_counts['Freq(Hz) (x1)'] = raw_synapse_counts['Freq(Hz) (x1)'].apply(lambda x: x*1000)\n",
    "raw_synapse_counts.rename(columns={'Case':'Subject', 'IHCs' : 'IHCs (y2)'}, inplace=True)\n",
    "\n",
    "paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(Hz) (x1)']), on=['Subject', 'Freq(Hz) (x1)'])\n",
    "slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(Hz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final = paired[['Subject', 'Freq(Hz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final_clean = final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15950326, -0.14409909, -0.13224509, ...,  2.84571385,\n",
       "        2.91967556,  2.98799673])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(amp_df_full['Amplitude (x3)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15950326, -0.14409909, -0.13224509, ...,  2.84571385,\n",
       "        2.91967556,  2.98799673])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final['Amplitude (x3)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Freq(Hz) (x1)</th>\n",
       "      <th>Level(dB) (x2)</th>\n",
       "      <th>Amplitude (x3)</th>\n",
       "      <th>vx (x4)</th>\n",
       "      <th>Synapses to IHC (y1)</th>\n",
       "      <th>IHCs (y2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>WPZ174</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>v1</td>\n",
       "      <td>12.549020</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>WPZ174</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>v2</td>\n",
       "      <td>13.444444</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>WPZ116</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.018918</td>\n",
       "      <td>v1</td>\n",
       "      <td>15.208333</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>WPZ116</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.018918</td>\n",
       "      <td>v2</td>\n",
       "      <td>14.949495</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>WPZ88</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-0.015700</td>\n",
       "      <td>v1</td>\n",
       "      <td>7.640449</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>WPZ100</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.093725</td>\n",
       "      <td>v2</td>\n",
       "      <td>18.461538</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>WPZ138</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>v1</td>\n",
       "      <td>15.773196</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>WPZ138</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>v2</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>WPZ139</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-0.004645</td>\n",
       "      <td>v1</td>\n",
       "      <td>4.631579</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>WPZ139</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-0.004645</td>\n",
       "      <td>v2</td>\n",
       "      <td>3.981481</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Subject  Freq(Hz) (x1)  Level(dB) (x2)  Amplitude (x3) vx (x4)  \\\n",
       "139   WPZ174        45200.0            40.0       -0.011515      v1   \n",
       "139   WPZ174        45200.0            40.0       -0.011515      v2   \n",
       "408   WPZ116        32000.0            30.0       -0.018918      v1   \n",
       "408   WPZ116        32000.0            30.0       -0.018918      v2   \n",
       "1076   WPZ88        45200.0            70.0       -0.015700      v1   \n",
       "...      ...            ...             ...             ...     ...   \n",
       "6608  WPZ100        16000.0            50.0       -0.093725      v2   \n",
       "6644  WPZ138         8000.0            25.0       -0.037091      v1   \n",
       "6644  WPZ138         8000.0            25.0       -0.037091      v2   \n",
       "7186  WPZ139        32000.0            80.0       -0.004645      v1   \n",
       "7186  WPZ139        32000.0            80.0       -0.004645      v2   \n",
       "\n",
       "      Synapses to IHC (y1) IHCs (y2)  \n",
       "139              12.549020      10.2  \n",
       "139              13.444444         9  \n",
       "408              15.208333       9.6  \n",
       "408              14.949495       9.9  \n",
       "1076              7.640449       8.9  \n",
       "...                    ...       ...  \n",
       "6608             18.461538       9.1  \n",
       "6644             15.773196       9.7  \n",
       "6644             19.000000         8  \n",
       "7186              4.631579       9.5  \n",
       "7186              3.981481      10.8  \n",
       "\n",
       "[70 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clean[final_clean['Amplitude (x3)'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, dropout_prob=0.1):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.fc1 = nn.Linear(32 * 61, 128)\n",
    "#         self.fc2 = nn.Linear(128, 1)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "#         x = self.dropout(x)\n",
    "#         x = x.view(-1, 32 * 61)\n",
    "#         x = nn.functional.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# peak_finding_model = CNN()\n",
    "# model_loader = torch.load('./models/waveI_cnn_model.pth')\n",
    "# peak_finding_model.load_state_dict(model_loader)\n",
    "# peak_finding_model.eval()\n",
    "\n",
    "# def peak_finding(wave):\n",
    "#     # Prepare waveform\n",
    "#     waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "#     # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "#     waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "#     # print(waveform_torch)\n",
    "#     # Get prediction from model\n",
    "#     outputs = peak_finding_model(waveform_torch)\n",
    "#     prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "#     # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "#     # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "#     # Apply Gaussian smoothing\n",
    "#     smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "#     # Find peaks and troughs\n",
    "#     n = 18\n",
    "#     t = 14\n",
    "#     # start_point = prediction - 9 archived ABRA\n",
    "#     start_point = prediction - 6 #newer ABRA\n",
    "#     smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "#     smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "#     sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "#     highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "#     relevant_troughs = np.array([])\n",
    "#     for p in range(len(highest_smoothed_peaks)):\n",
    "#         c = 0\n",
    "#         for t in smoothed_troughs:\n",
    "#             if t > highest_smoothed_peaks[p]:\n",
    "#                 if p != 4:\n",
    "#                     try:\n",
    "#                         if t < highest_smoothed_peaks[p+1]:\n",
    "#                             relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "#                             break\n",
    "#                     except IndexError:\n",
    "#                         pass\n",
    "#                 else:\n",
    "#                     relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "#                     break\n",
    "#     relevant_troughs = relevant_troughs.astype('i')\n",
    "#     return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "# import pandas as pd\n",
    "# import io\n",
    "# import re\n",
    "\n",
    "# def extract_metadata(metadata_lines):\n",
    "#     # Dictionary to store extracted metadata\n",
    "#     metadata = {}\n",
    "    \n",
    "#     for line in metadata_lines:\n",
    "#         # Extract SW FREQ\n",
    "#         freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "#         if freq_match:\n",
    "#             metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "#         # Extract LEVELS\n",
    "#         levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "#         if levels_match:\n",
    "#             # Split levels and convert to list of floats\n",
    "#             metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "#     return metadata\n",
    "\n",
    "# def read_custom_tsv(file_path):\n",
    "#     # Read the entire file\n",
    "#     with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "#         content = f.read()\n",
    "    \n",
    "#     # Split the content into metadata and data sections\n",
    "#     metadata_lines = []\n",
    "#     data_section = None\n",
    "    \n",
    "#     # Find the ':DATA' marker\n",
    "#     data_start = content.find(':DATA')\n",
    "    \n",
    "#     if data_start != -1:\n",
    "#         # Extract metadata (lines before ':DATA')\n",
    "#         metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "#         # Extract data section\n",
    "#         data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "#     # Extract specific metadata\n",
    "#     metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "#     # Read the data section directly\n",
    "#     try:\n",
    "#         # Use StringIO to create a file-like object from the data section\n",
    "#         raw_data = pd.read_csv(\n",
    "#             io.StringIO(data_section), \n",
    "#             sep='\\s+',  # Use whitespace as separator\n",
    "#             header=None\n",
    "#         )\n",
    "#         raw_data = raw_data.T\n",
    "#         # Add metadata columns to the DataFrame\n",
    "#         if 'SW_FREQ' in metadata:\n",
    "#             raw_data['Freq(Hz)'] = metadata['SW_FREQ']\n",
    "#             raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "#         if 'LEVELS' in metadata:\n",
    "#             # Repeat levels to match the number of rows\n",
    "#             levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "#             raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "#         filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "#         filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "#         columns = ['Freq(Hz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "#         filtered_data = filtered_data[columns]\n",
    "#         return filtered_data\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading data: {e}\")\n",
    "#         return None, metadata\n",
    "\n",
    "# # Use the function\n",
    "# # abr_test = \"/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/wpz_104L/11_3/ABR-104-L-11.3.tsv\"\n",
    "# # raw_data = read_custom_tsv(abr_test)\n",
    "\n",
    "# def get_str(data):\n",
    "#     # return string up until null character only\n",
    "#     ind = data.find(b'\\x00')\n",
    "#     if ind > 0:\n",
    "#         data = data[:ind]\n",
    "#     return data.decode('utf-8')\n",
    "\n",
    "# def interpolate_and_smooth(final, target_length=244): # To implement after moving beyond Manor data since we already have 244 time points\n",
    "#     if len(final) > target_length:\n",
    "#         new_points = np.linspace(0, len(final), target_length + 2)\n",
    "#         interpolated_values = np.interp(new_points, np.arange(len(final)), final)\n",
    "#         final = np.array(interpolated_values[:target_length], dtype=float)\n",
    "#     elif len(final) < target_length:\n",
    "#         original_indices = np.arange(len(final))\n",
    "#         target_indices = np.linspace(0, len(final) - 1, target_length)\n",
    "#         cs = CubicSpline(original_indices, final)\n",
    "#         final = cs(target_indices)\n",
    "#     return final\n",
    "\n",
    "# def peaks_troughs_amp_final(df, freq, db, time_scale=10):\n",
    "#     khz = df[(df['Freq(Hz)'] == freq) & (df['Level(dB)'] == db)]\n",
    "#     if not khz.empty:\n",
    "#         index = khz.index.values[0]\n",
    "#         final = df.loc[index, '0':].dropna()\n",
    "#         final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "#         target = int(244 * (time_scale / 10))\n",
    "        \n",
    "#         y_values = interpolate_and_smooth(final, target)  # Original y-values for plotting\n",
    "#         sampling_rate = len(y_values) / time_scale\n",
    "\n",
    "#         x_values = np.linspace(0, len(y_values) / sampling_rate, len(y_values))\n",
    "\n",
    "#         y_values = interpolate_and_smooth(final[:244])\n",
    "\n",
    "#         fpf = df[(df['Freq(Hz)'] == freq)].loc[:, '0':]\n",
    "\n",
    "#         # Flatten the data to scale all values across the group\n",
    "#         flattened_data = fpf.values.flatten().reshape(-1, 1)\n",
    "\n",
    "#         # Step 1: Standardize the data\n",
    "#         scaler = StandardScaler()\n",
    "#         standardized_data = scaler.fit_transform(flattened_data)\n",
    "\n",
    "#         # Step 2: Apply min-max scaling\n",
    "#         min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#         scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(fpf.shape)\n",
    "\n",
    "#         # Reshape back to the original shape and update the group\n",
    "#         fpf[fpf.columns] = scaled_data\n",
    "\n",
    "#         finalfpf = fpf.loc[index, '0':].dropna()\n",
    "#         finalfpf = pd.to_numeric(finalfpf, errors='coerce').dropna()\n",
    "\n",
    "#         target = int(244 * (time_scale / 10))\n",
    "        \n",
    "#         y_values_fpf = interpolate_and_smooth(finalfpf, target)  # Original y-values for plotting\n",
    "#         sampling_rate = len(y_values) / time_scale\n",
    "\n",
    "#         y_values_fpf = interpolate_and_smooth(finalfpf[:244])\n",
    "\n",
    "#         highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "\n",
    "#         if highest_peaks.size > 0:  # Check if highest_peaks is not empty\n",
    "#             first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "\n",
    "#         return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "#     return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_scale=10\n",
    "# amp_per_freq = {'Subject': [], 'Freq(Hz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "# start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "# for subject in os.listdir(start_path):\n",
    "#     # print(\"Subject:\",subject)\n",
    "#     for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "#         # print(fq)\n",
    "#         if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "#             path = os.path.join(start_path,subject,fq)\n",
    "#             data_df = read_custom_tsv(path)\n",
    "#             freqs = data_df['Freq(Hz)'].unique().tolist()\n",
    "#             levels = data_df['Level(dB)'].unique().tolist()\n",
    "#             for freq in freqs:\n",
    "#                 for lvl in levels:\n",
    "#                     # print(\"Frequency=\",freq, \"Level=\", lvl)\n",
    "#                     _, _, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl)\n",
    "#                     # print(f'Amplitude: {amp}\\n')\n",
    "#                     amp_per_freq['Subject'].append(subject)\n",
    "#                     amp_per_freq['Freq(Hz) (x1)'].append(freq)\n",
    "#                     amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "#                     amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "#         else:\n",
    "#             pass\n",
    "#         # print(path)\n",
    "\n",
    "# amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "# raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "# raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna()\n",
    "# raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "# raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "# raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "# raw_synapse_counts.rename(columns={'Freq':'Freq(Hz) (x1)'}, inplace=True)\n",
    "# raw_synapse_counts['Freq(Hz) (x1)'] = raw_synapse_counts['Freq(Hz) (x1)'].apply(lambda x: x*1000)\n",
    "# raw_synapse_counts.rename(columns={'Case':'Subject', 'IHCs' : 'IHCs (y2)'}, inplace=True)\n",
    "\n",
    "# paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(Hz) (x1)']), on=['Subject', 'Freq(Hz) (x1)'])\n",
    "# slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(Hz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "# final = paired[['Subject', 'Freq(Hz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "# final_clean = final.dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_manorimagepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
